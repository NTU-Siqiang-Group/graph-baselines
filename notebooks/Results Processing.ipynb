{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result  Visualization Notebook\n",
    "\n",
    "The following assemble data and produces the charts presented in:\n",
    "\n",
    "> Beyond Macrobenchmarks: Microbenchmark-based Graph Database Evaluation.\n",
    "  by Lissandrini, Matteo; Brugnara, Martin; and Velegrakis, Yannis.\n",
    "  In PVLDB, 12(4):390-403, 2018. \n",
    "  \n",
    "This notebook can process experiments incrementally, so you can use the notebook while experiments are still running, replaying the notebook should produce updated charts.\n",
    "\n",
    "**Note:** in the following we assume that missing experiments imply the system has reached the time-out.\n",
    "\n",
    "## Quick References\n",
    "\n",
    " - [Charts](#charts)\n",
    "     - [Microbenchmark](#query_time)\n",
    "     - [Macrobenchmark](#macrobench)\n",
    "     - [Cumulative Time: Single vs Batch](#time_sum)\n",
    "     - [Effect of indexes](#index_time)\n",
    "     - [Disk Space](#disk)\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting fresh each time.\n",
    "import os\n",
    "if os.path.exists('results.db'):\n",
    "    os.remove('results.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "from collections import OrderedDict\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "\n",
    "def get_default_db():\n",
    "    return DbSession('results.db')\n",
    "\n",
    "\n",
    "class DbSession(object):\n",
    "    def __init__(self, conn_str):\n",
    "        self.conn = sqlite3.connect(conn_str)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self.cursor\n",
    "    \n",
    "    def __exit__(self, t, value, traceback):\n",
    "        if t is not None:\n",
    "            log(traceback)\n",
    "        self.cursor.close()\n",
    "        self.conn.commit()\n",
    "        self.conn.close()\n",
    "\n",
    "\n",
    "def log(msg):\n",
    "    import datetime\n",
    "    print('%s - %s' % (datetime.datetime.now().isoformat(), msg))\n",
    "\n",
    "    \n",
    "def gby(lst, key=0):\n",
    "    \"\"\" Group by that respect keys order.\n",
    "    \"\"\"\n",
    "    from collections import OrderedDict\n",
    "    res = OrderedDict()\n",
    "    for item in lst:\n",
    "        k = item[key]\n",
    "        if k in res: \n",
    "            res[k].append(item)\n",
    "        else: \n",
    "            res[k] = [item]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap \n",
    "Let's first **create**, **intialize** and then **load** the data from the `*.csv` file.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_list = ['results', 'queries', 'datasets', 'images', 'dborder']\n",
    "\n",
    "with get_default_db() as c:\n",
    "    # Reset database\n",
    "    stmt = 'DROP TABLE IF EXISTS '\n",
    "    for tbl in tables_list:\n",
    "        c.execute(stmt + tbl)\n",
    "\n",
    "# Setup database\n",
    "create_tables = [\n",
    "    '''\n",
    "    -- Only common fields\n",
    "    CREATE TABLE IF NOT EXISTS results (\n",
    "            -- _rowid_ (https://www.sqlite.org/lang_createtable.html#rowid)\n",
    "            date       INTEGER NOT NULL, \n",
    "            dbengine   TEXT NOT NULL, \n",
    "            dataset    TEXT NOT NULL,     -- please store just name, not path\n",
    "            query      TEXT NOT NULL, \n",
    "            sid        INTEGER,\n",
    "            int_order  INTEGER,\n",
    "            exec_time  INTEGER NOT NULL\n",
    "    )\n",
    "\n",
    "    ''',\n",
    "    '''\n",
    "    CREATE TABLE IF NOT EXISTS queries (\n",
    "            -- _rowid_ (https://www.sqlite.org/lang_createtable.html#rowid)\n",
    "            query      TEXT, \n",
    "            cnt        INTEGER, -- expected number of queries\n",
    "            cls        TEXT,\n",
    "            qord       int,\n",
    "            paper_id   TEXT\n",
    "    );\n",
    "    ''',\n",
    "    '''\n",
    "    CREATE TABLE IF NOT EXISTS datasets (\n",
    "            -- _rowid_ (https://www.sqlite.org/lang_createtable.html#rowid)\n",
    "            dataset TEXT, \n",
    "            json2   REAL,\n",
    "            dord    INT\n",
    "    );\n",
    "    ''',\n",
    "    '''\n",
    "    -- Images size on disk\n",
    "    CREATE TABLE IF NOT EXISTS images (\n",
    "            -- _rowid_ (https://www.sqlite.org/lang_createtable.html#rowid)\n",
    "            dbengine      TEXT, \n",
    "            dataset       TEXT, -- Special value 'empty'\n",
    "            space         REAL\n",
    "    );\n",
    "    ''',\n",
    "    '''\n",
    "    -- Defined ordering\n",
    "    CREATE TABLE IF NOT EXISTS dborder (\n",
    "            -- _rowid_ (https://www.sqlite.org/lang_createtable.html#rowid)\n",
    "            dbengine      TEXT, \n",
    "            dbalias       TEXT,\n",
    "            ord           int \n",
    "    );\n",
    "    ''',\n",
    "]\n",
    "\n",
    "assert(len(tables_list) == len(create_tables))\n",
    "    \n",
    "with get_default_db() as c:\n",
    "    for tbl in create_tables:\n",
    "        c.execute(tbl)\n",
    "    \n",
    "\n",
    "log('Database intialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixtures\n",
    "While the following meta data may be derived from the queries themselves,\n",
    "hardcoding them allows us to simplify the presentation.  \n",
    "See for example, how we deal with the BFS length parameters in the _insert_\\__result()_ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries\n",
    "Comment any of the following tuples to avoid analizing the specific queries.\n",
    "Please remeber the implicit assumption that if there is no _result_ entry then it is a timeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Meta array format:\n",
    "(QueryClass, [\n",
    "  ('query file name wo .rb', <expected # of results for run>, '<id_on_paper>')\n",
    "])\n",
    "\"\"\"\n",
    "meta = [\n",
    "    ('L', [('loader', 1, '1')]),\n",
    "    ('C', [\n",
    "        ('insert-node', 10, '2'),\n",
    "        ('insert-edge', 10, '3'),\n",
    "        ('insert-edge-with-property', 10, '4'),\n",
    "        ('insert-node-property', 10, '5'),\n",
    "        ('insert-edge-property', 10, '6'),\n",
    "        ('insert-node-with-edges', 10, '7'),\n",
    "    ]),\n",
    "    ('R', [\n",
    "        ('count-nodes', 1, '8'),\n",
    "        ('count-edges', 1, '9'),\n",
    "        ('find-unique-labels', 1, '10'),\n",
    "        ('node-property-search', 10, '11'),\n",
    "        ('edge-specific-property-search', 10, '12'),\n",
    "        ('label-search', 10, '13'),\n",
    "        ('id-search-node', 10, '14'),\n",
    "        ('id-search-edge', 10, '15'),\n",
    "    ]),\n",
    "    ('U', [\n",
    "        ('update-node-property', 10, '16'),\n",
    "        ('update-edge-property', 10, '17'),\n",
    "    ]),\n",
    "    ('D', [\n",
    "        ('delete-nodes', 10, '18'),\n",
    "        ('delete-edges', 10, '19'),\n",
    "        ('delete-node-property', 10, '20'),\n",
    "        ('delete-edge-property', 10, '21'),        \n",
    "    ]),\n",
    "    ('T', [\n",
    "        ('NN-incoming', 10, '22'),\n",
    "        ('NN-outgoing', 10, '23'),        \n",
    "        ('NN-both-filtered', 10, '24'),   \n",
    "        ('NN-incoming-unique-label', 10, '25'),\n",
    "        ('NN-outgoing-unique-label', 10, '26'),\n",
    "        ('NN-both-unique-label', 10, '27'),\n",
    "        ('k-degree-in', 1, '28'),\n",
    "        ('k-degree-out', 1, '29'),\n",
    "        ('k-degree-both', 1, '30'),\n",
    "        ('find-non-root-nodes', 1, '31'),\n",
    "        # BFS, and BFS-labelled would go here\n",
    "        ('shortest-path', 10, '34'),\n",
    "        ('shortest-path-labelled', 10, '35'),\n",
    "    ]),  \n",
    "    ('B', [\n",
    "        # 32\n",
    "        ('BFS2', 10, ''),\n",
    "        ('BFS3', 10, ''),\n",
    "        ('BFS4', 10, ''),\n",
    "        ('BFS5', 10, ''),\n",
    "        # 33\n",
    "        ('BFS-labelled2', 10, ''),\n",
    "        ('BFS-labelled3', 10, ''),\n",
    "        ('BFS-labelled4', 10, ''),\n",
    "        ('BFS-labelled5', 10, ''),\n",
    "    ]),\n",
    "    \n",
    "    ('I', [\n",
    "        # (\"X-create-index\", 1, ''),\n",
    "        (\"X-insert-node\", 10,          '2'),\n",
    "        (\"X-insert-node-property\", 10, '5'),\n",
    "        (\"X-node-property-search\", 10, '11'),\n",
    "        (\"X-update-node-property\", 10, '16'),\n",
    "        (\"X-delete-nodes\", 10,         '18'),\n",
    "        (\"X-delete-node-property\", 10, '20'),\n",
    "    ]),\n",
    "    \n",
    "    ('S', [\n",
    "        # (\"ldbc-create-index\", 1, ''),\n",
    "        (\"ldbc-macrobench-max-iid\", 10, ''),\n",
    "        (\"ldbc-macrobench-max-oid\", 10, ''),\n",
    "        (\"ldbc-macrobench-create\", 10, ''),\n",
    "        (\"ldbc-macrobench-city\", 10, ''),\n",
    "        (\"ldbc-macrobench-company\", 10, ''),\n",
    "        (\"ldbc-macrobench-university\", 10, ''),\n",
    "        (\"ldbc-macrobench-friend1\", 10, ''),\n",
    "        (\"ldbc-macrobench-friend2\", 10, ''),\n",
    "        (\"ldbc-macrobench-friend-tags\", 10, ''),\n",
    "        (\"ldbc-macrobench-add-tags\", 10, ''),\n",
    "        (\"ldbc-macrobench-friend-of-friend\", 10, ''),\n",
    "        (\"ldbc-macrobench-triangle-closure\", 10, ''),\n",
    "        (\"ldbc-macrobench-places\", 10, ''),\n",
    "    ]),\n",
    "]\n",
    "\n",
    "# Add queries in order \n",
    "insert = 'INSERT INTO queries VALUES (?, ?, ?, ?, ?);'\n",
    "with get_default_db() as c:\n",
    "    qord = 0\n",
    "    for (cls, queries) in  meta:\n",
    "        for (q, cnt, paper_id) in  queries:\n",
    "            c.execute(insert, [q, cnt, cls, qord, paper_id])\n",
    "            qord += 1\n",
    "            \n",
    "log('Queries meta have been loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databases\n",
    "# NOTE: first index-capables then others.\n",
    "dborder = [\n",
    "    (\"neo4j\",      \"Neo4j 1.9\"),\n",
    "    (\"neo4j-tp3\",  \"Neo4j 3.0\"),\n",
    "    (\"orientdb\",   \"OrientDB\"),\n",
    "    (\"titan\",      \"Titan 0.5\"),\n",
    "    (\"titan-tp3\",  \"Titan 1.0\"),\n",
    "    (\"sparksee\",   \"Sparksee\"),\n",
    "    (\"arangodb\",   \"ArangoDB\"), \n",
    "    (\"pg\",         \"Sqlg\"),\n",
    "    (\"blazegraph\", \"Blazegraph\"),\n",
    "]\n",
    "\n",
    "with get_default_db() as c:\n",
    "    for (i,d) in enumerate(dborder):\n",
    "        c.execute('INSERT INTO dborder VALUES (?,?,?)', (*d, i))\n",
    "\n",
    "log('Fixture loadings completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'INSERT INTO datasets VALUES (?,?,?)'\n",
    "with get_default_db() as c:\n",
    "    with open('datasets.tsv') as f:\n",
    "        i = 0\n",
    "        for l in f:\n",
    "            size, ds = l.split()\n",
    "            c.execute(q, [ds, float(size)/1024, i])\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'INSERT INTO images VALUES (?,?,?)'\n",
    "with get_default_db() as c:\n",
    "    with open('images.csv') as f:\n",
    "        for l in f:\n",
    "            name, size = l.split(',')\n",
    "            if '_' in name:\n",
    "                db, ds = name.strip()[8:-6].split('_',1)\n",
    "            else:\n",
    "                db, ds = name.strip()[8:], 'empty'\n",
    "            size = size.strip()\n",
    "            size = float(size[:-2]) * (1024 if size[-2] == 'G' else 1)\n",
    "            c.execute(q, [db, ds, size]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The following import support importing results of experiments carried out on different time or machines. It is up to the researche to guarantee the consistency of the external environemnt, _e.g._ when usign multiple machine, they must make sure the machines are identical in hardware, software, and configuration.\n",
    "\n",
    "See _collect.sh_ for more detail about results harvesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to results directory\n",
    "RES_DIR='/results/'\n",
    "\n",
    "# Get the list of the experiment results available on disk.\n",
    "exps = set(fn.split('_',1)[0]\n",
    "           for fn in os.listdir(RES_DIR)\n",
    "           if os.path.isfile(path.join(RES_DIR, fn)) \n",
    "               and fn[0] is not '.')\n",
    "log('Importing: {}'.format(exps))\n",
    "\n",
    "# Get list of experiments executed with indexes.\n",
    "indexed_exps = []\n",
    "if os.path.exists('indexed.csv'):\n",
    "    with open('indexed.csv') as f:\n",
    "        indexed_exps = set(t.strip() for t in f)\n",
    "log('Experiments with indexes: {}'.format(indexed_exps))\n",
    "\n",
    "\n",
    "# Support function\n",
    "def insert_result(c, timestamp, row, indexed_exps):    \n",
    "    insert_res = 'INSERT INTO results VALUES (?, ?, ?, ?, ?, ?, ?);'\n",
    "    \n",
    "    # NOTE: we use replace instead of substr so it works also for LDBC.\n",
    "    query = row[2].split('/')[-1].replace('.groovy', '').replace('bulk', '')\n",
    "    if query.startswith('BFS'):\n",
    "        query += row[-1]\n",
    "    exec_time = int(row[6])\n",
    "    \n",
    "    if timestamp in indexed_exps:\n",
    "        query = 'X-' + query\n",
    "\n",
    "    if query.startswith('loader'):\n",
    "        exec_time *= 1000000.0\n",
    "    \n",
    "    param = [\n",
    "        timestamp,\n",
    "        row[0][len('gremlin-'):],\n",
    "        row[1].split('/')[-1].replace('_hashed', '').replace('_noslash.json', '').rstrip('2')[:-len('.json')],\n",
    "        query,\n",
    "        row[3] or '-1', # sid\n",
    "        row[5], # int_order\n",
    "        exec_time,\n",
    "    ]\n",
    "    c.execute(insert_res, param)\n",
    "    \n",
    "\n",
    "with get_default_db() as c:\n",
    "    for res in [fn for fn in os.listdir(RES_DIR)\n",
    "                if os.path.isfile(path.join(RES_DIR, fn))\n",
    "                and fn[0] is not '.' and fn.split('_',2)[0] in exps]:\n",
    "        with open(path.join(RES_DIR, res), 'r') as csvfile:\n",
    "            if not res[-len('results.csv'):] == 'results.csv':\n",
    "                continue\n",
    "            reader = (row for row in csv.reader(csvfile) if row[0].startswith('gremlin-'))\n",
    "            for row in reader:\n",
    "                insert_result(c, res.split('_',2)[0], row, indexed_exps)\n",
    "log('Results import process completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Views \n",
    "Define several views used to make analitics' SQL for the _Charts_ less obscure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_names = [\n",
    "    'one_per_row', 'single', 'bulk', 'dbs',\n",
    "    'full_queries_set', 'single_means_wtimeout', \n",
    "    'bulk_sum_wtimeout', 'healt', 'healt_bulk',\n",
    "]\n",
    "\n",
    "views = [\n",
    "    # Results normalization:\n",
    "    # sum the query that reports the exec_time splitted in multiple record.\n",
    "    # NOTE, here we make the assumption (even if not 100% valid) \n",
    "    #       that all \"pieces\" are in the same result file ('date'),\n",
    "    #       and that a query is not run more than once\n",
    "    #       in the same experiment ('date')\n",
    "    # NOTE, \"-tp3\" may have been colleted both in `$DB` and `${DB}-tp3`\n",
    "    #       results. Thus use DISTINCT.\n",
    "    '''\n",
    "      CREATE VIEW one_per_row AS \n",
    "        SELECT dbengine,dataset,query,sid,int_order,\n",
    "            CASE \n",
    "                WHEN COUNT(*) >= 5 THEN\n",
    "                    (SUM(exec_time) - MIN(exec_time) - MAX(exec_time)) / (COUNT(*)-2)\n",
    "                ELSE\n",
    "                    AVG(exec_time)\n",
    "            END AS exec_time\n",
    "        FROM (\n",
    "            -- already ok\n",
    "            SELECT DISTINCT * FROM results\n",
    "            WHERE \n",
    "                (query <> 'insert-node-with-edges') AND\n",
    "                (dbengine <> \"arangodb\" OR query <> 'loader')\n",
    "\n",
    "            UNION\n",
    "\n",
    "            -- insert-node-with-edges.groovy & arango loader        \n",
    "            SELECT date,dbengine,dataset,query,sid,int_order,SUM(exec_time)\n",
    "            FROM results\n",
    "            WHERE \n",
    "                query = 'insert-node-with-edges' OR\n",
    "                (dbengine = \"arangodb\" AND query = 'loader')\n",
    "            GROUP BY date,dbengine,dataset,query,sid,int_order  \n",
    "            \n",
    "            UNION\n",
    "            \n",
    "            SELECT date,dbengine,dataset,'ldbc-create-user',sid,int_order,SUM(exec_time)\n",
    "            FROM results\n",
    "            WHERE query LIKE 'ldbc-create-user%'\n",
    "            GROUP BY date,dbengine,dataset,sid,int_order\n",
    "                \n",
    "        )\n",
    "        GROUP BY dbengine,dataset,query,sid,int_order  \n",
    "    ''',\n",
    "    \n",
    "    # Run that are NOT bulk mode\n",
    "    '''\n",
    "        CREATE VIEW single AS\n",
    "            SELECT DISTINCT * FROM one_per_row WHERE int_order = 0 OR int_order = ''\n",
    "    ''',\n",
    "    \n",
    "    # ONLY run that are bulk mode\n",
    "    '''\n",
    "        CREATE VIEW bulk AS\n",
    "            SELECT DISTINCT * FROM one_per_row WHERE NOT (int_order = 0 OR int_order = '')\n",
    "    ''',\n",
    "    \n",
    "    # dbengines view (based on images size)\n",
    "    '''\n",
    "        CREATE VIEW dbs AS\n",
    "            SELECT DISTINCT dbengine \n",
    "            FROM images\n",
    "    ''',\n",
    "    \n",
    "    # Expected query set (wo/int_order)\n",
    "    '''\n",
    "        CREATE VIEW full_queries_set AS\n",
    "            SELECT dbengine,dataset,query,cnt\n",
    "            FROM dbs, queries, datasets\n",
    "    ''',\n",
    "       \n",
    "    # single_means_wtimeout\n",
    "    # Assumption: if COUNT() < expected --> timedout\n",
    "    # SUM(query) + timeout (:= 2h) * (Expected query count - COUNT(query))\n",
    "    '''\n",
    "        CREATE VIEW single_means_wtimeout AS\n",
    "            SELECT dbengine,dataset,q.query,\n",
    "                (SUM(exec_time/1000000) + (7200000 * max(0,q.cnt - SUM(done))))/q.cnt AS exec_time,\n",
    "                q.cnt, SUM(done) AS done\n",
    "            FROM (\n",
    "                SELECT dbengine,dataset,query,exec_time,1 AS done\n",
    "                FROM single\n",
    "                \n",
    "                UNION\n",
    "                \n",
    "                SELECT dbengine,dataset,query,0,0\n",
    "                FROM full_queries_set\n",
    "            ) AS r\n",
    "            NATURAL JOIN queries AS q\n",
    "            GROUP BY dbengine,dataset,query\n",
    "    ''',\n",
    "    \n",
    "    # bulk_sum_wtimeout\n",
    "    # Assumption: if COUNT() < expected --> timedout\n",
    "    '''\n",
    "        CREATE VIEW bulk_sum_wtimeout AS\n",
    "            SELECT dbengine,dataset,r.query,\n",
    "                CASE\n",
    "                    WHEN SUM(done) <> q.cnt THEN 7200000\n",
    "                    ELSE SUM(exec_time) / 1000000\n",
    "                END AS exec_time,\n",
    "                q.cnt, SUM(done) AS done\n",
    "            FROM (\n",
    "                SELECT dbengine,dataset,query,exec_time,1 AS done\n",
    "                FROM bulk\n",
    "                \n",
    "                UNION\n",
    "                \n",
    "                SELECT dbengine,dataset,query,0,0\n",
    "                FROM full_queries_set\n",
    "                WHERE cnt > 1\n",
    "            ) AS r\n",
    "            NATURAL JOIN queries AS q\n",
    "            GROUP BY dbengine,dataset,query\n",
    "    ''',\n",
    "    \n",
    "    # health\n",
    "    # Checks how many queries has been completed and how many timedout.\n",
    "    '''\n",
    "    CREATE VIEW health AS\n",
    "        SELECT dbengine, dataset, query,\n",
    "            -- NOTE, count on any of \"single\" field, but not on *. \n",
    "            -- Remember that we are using a left natual join.\n",
    "            -- Assumption, NOT NULL\n",
    "            COUNT(single.query) AS cnt, \n",
    "            \n",
    "            -- NOTE, assumpion SID is always present (NOT NULL, DEFAULT '-1')\n",
    "            COUNT(DISTINCT sid) AS sid_cnt,\n",
    "            cnt AS expected_cnt,\n",
    "            CASE      \n",
    "                WHEN COUNT(DISTINCT sid) < cnt\n",
    "                    THEN \"missing\"\n",
    "                WHEN COUNT(DISTINCT sid) > cnt\n",
    "                    THEN \"wat!\"\n",
    "                WHEN COUNT(DISTINCT sid) <>  COUNT(*)\n",
    "                    THEN \"sid inconsistence\"\n",
    "                ELSE \"ok\"\n",
    "            END AS status                   \n",
    "        FROM full_queries_set\n",
    "        LEFT NATURAL JOIN single\n",
    "        GROUP BY dbengine, dataset, query\n",
    "    ''',\n",
    "    \n",
    "    # healt_bulk\n",
    "    # Checks how many queries has been completed and how many timedout.\n",
    "    '''\n",
    "    CREATE VIEW health_bulk AS\n",
    "        SELECT dbengine, dataset, query,\n",
    "            COUNT(bulk.query) AS cnt,\n",
    "            COUNT(DISTINCT int_order) AS sid_cnt,\n",
    "            cnt AS expected_cnt,\n",
    "            CASE      \n",
    "                WHEN COUNT(DISTINCT int_order) < cnt\n",
    "                    THEN \"missing\"\n",
    "                WHEN COUNT(DISTINCT int_order) > cnt\n",
    "                    THEN \"wat!\"\n",
    "                WHEN COUNT(DISTINCT int_order) <>  COUNT(*)\n",
    "                    THEN \"sid inconsistence\"\n",
    "                ELSE \"ok\"\n",
    "            END AS status                   \n",
    "        FROM full_queries_set\n",
    "        LEFT NATURAL JOIN bulk\n",
    "        -- queries with cnt == 1 does not have bulk version\n",
    "        WHERE cnt > 1 \n",
    "        GROUP BY dbengine, dataset, query        \n",
    "    ''',\n",
    "]\n",
    "\n",
    "assert len(views_names) is len(views)\n",
    "\n",
    "with get_default_db() as c:\n",
    "    stmt = 'DROP VIEW IF EXISTS %s'\n",
    "    for v in views_names:\n",
    "        c.execute(stmt % v)\n",
    "    log(\"All views have been dropped\")\n",
    "\n",
    "    for v in views:\n",
    "        c.execute(v)   \n",
    "log(\"All views have been recreated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='charts'></a>\n",
    "# Charts\n",
    "\n",
    "Below all the plots of the experiments on running time comparison.\n",
    "For all the following charts, lower values are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Style from paper.\n",
    "styles = [\n",
    "    {'color':'blue',      'linestyle':':',  'marker': 'x'}, \n",
    "    {'color':'cyan',      'linestyle':':',  'marker': 'o'},\n",
    "    {'color':'gold',      'linestyle':'-',  'marker': 'D'},\n",
    "    {'color':'olive',     'linestyle':'-',  'marker': 'x'},\n",
    "    {'color':'darkgreen', 'linestyle':':',  'marker': '+'},\n",
    "    {'color':'black',     'linestyle':'-.', 'marker': '*'},\n",
    "    {'color':'red',       'linestyle':'--', 'marker': '^'},\n",
    "    {'color':'grey',      'linestyle':':',  'marker': 'v'},\n",
    "    {'color':'green',     'linestyle':'-',  'marker': 's'},\n",
    "]\n",
    "markers = {'markersize': 8, 'fillstyle': 'none'}\n",
    "\n",
    "# Make it readable\n",
    "plt.rcParams.update({'figure.figsize': (20,10), 'font.size': 20})\n",
    "\n",
    "\n",
    "def ds_short(ds_name):\n",
    "    if ds_name[:len('freebase')] == 'freebase':\n",
    "        return 'FBR-' + ds_name.split('_')[1][0].upper()\n",
    "    return ' '.join(ds_name.upper().split())    \n",
    "\n",
    "\n",
    "re_qpff = re.compile('[^a-zA-Z0-9]')\n",
    "def q_prettify(q):\n",
    "    return re_qpff.sub(' ', q)\n",
    "\n",
    "\n",
    "def get_queries(not_cls=list()):\n",
    "    \"\"\" Queries with at least one experiment,\n",
    "        in selcted classes (not_cls).\n",
    "    \"\"\"\n",
    "    with get_default_db() as c:\n",
    "        q = '''\n",
    "        SELECT DISTINCT cls, s.query, paper_id || ' ' || s.query\n",
    "        FROM single s\n",
    "        JOIN queries as q ON q.query = s.query AND cls NOT IN ({})\n",
    "        ORDER BY qord\n",
    "        '''\n",
    "        qf = q.format(','.join(map(lambda x: \"'\" + x + \"'\", not_cls)))\n",
    "        queries = [(r[0], r[1]) for r in c.execute(qf)]\n",
    "        queries_labels = [q_prettify(r[2].strip()) for r in c.execute(qf)]\n",
    "        in_queries = ','.join(map(lambda x: \"'\" + x[1] + \"'\", queries))\n",
    "        return queries, queries_labels, in_queries\n",
    "\n",
    "        \n",
    "# Datasets with at least one experiment\n",
    "with get_default_db() as c:\n",
    "    q= '''\n",
    "    SELECT DISTINCT dataset\n",
    "    FROM single\n",
    "    NATURAL JOIN datasets\n",
    "    ORDER BY dord\n",
    "    '''\n",
    "    datasets = [r[0] for r in c.execute(q)]\n",
    "in_datasets = ','.join(map(lambda x: \"'\" + x + \"'\", datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='query_time'></a>\n",
    "## Microbenchmark Queries\n",
    "The plots shows for the mean execution time for each query,\n",
    "where timeouts are counted as the timeout it self, 2h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plots all queries excpet these with specialized plots\n",
    "q = '''\n",
    "    SELECT dbalias, dataset, ROUND(exec_time, 2)        \n",
    "    FROM single_means_wtimeout as t\n",
    "    JOIN queries as q ON q.query = t.query\n",
    "    NATURAL JOIN dborder\n",
    "    NATURAL JOIN datasets AS ds\n",
    "    WHERE q.query='{}' AND dataset IN ({}) \n",
    "        -- Exclude classes with specialized plots\n",
    "        AND q.cls NOT IN ('B', 'S', 'I')\n",
    "    ORDER BY ord, dord\n",
    "'''\n",
    "\n",
    "queries, queries_labels, in_queries = get_queries(['B', 'S', 'I'])\n",
    "    \n",
    "xlabels = list(map(ds_short, datasets))\n",
    "current_cls = ''\n",
    "with get_default_db() as c:\n",
    "    for i, x in enumerate(queries):\n",
    "        cls, query = x\n",
    "        if cls != current_cls:\n",
    "            current_cls = cls\n",
    "            display(HTML('<h1 id=\"current_cls\">{}</h1>'.format(current_cls)))\n",
    "            \n",
    "        plt.title('#'+queries_labels[i].upper())\n",
    "        plt.xlabel('Dataset')\n",
    "        plt.ylabel('Time (ms)')\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True)\n",
    "    \n",
    "        rs = [r for r in c.execute(q.format(query, in_datasets))]\n",
    "        for i, r in enumerate(gby(rs).items()):\n",
    "            db, items = r\n",
    "            ts  = [x[2] for x in items]\n",
    "            plt.plot(ts, label=db, **styles[i%len(styles)], **markers)\n",
    "\n",
    "        plt.xticks(np.arange(len(xlabels)), xlabels, rotation=90)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "q = '''\n",
    "SELECT dbalias, t.query, ROUND(exec_time, 2)        \n",
    "FROM single_means_wtimeout as t\n",
    "JOIN queries as q ON q.query = t.query\n",
    "NATURAL JOIN dborder\n",
    "WHERE \n",
    "  -- select class and propoer datasets\n",
    "  q.query LIKE 'BFS-labelled%' AND dataset = 'ldbc' \n",
    "ORDER BY ord, qord\n",
    "'''\n",
    "\n",
    "plt.title('#33 BFS-labelled on LDBC')\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "with get_default_db() as c:\n",
    "    rs = list(c.execute(q)) \n",
    "\n",
    "llabels = None\n",
    "for i, r in enumerate(gby(rs).items()):\n",
    "    db, items = r\n",
    "    ts  = [x[2] for x in items]\n",
    "    if not llabels:\n",
    "        llabels = ['DEPTH ' + x[1][len('BFS-labelled'):] for x in items]\n",
    "    plt.plot(ts, label=db, **styles[i%len(styles)], **markers)\n",
    "     \n",
    "plt.xticks(np.arange(len(llabels)), llabels, rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='macrobench'></a>\n",
    "## LDBC Marcobenchmark\n",
    "\n",
    "The queries for the Macrobenchmark are executed alltogether by a single query file on top of an image with ad-hoc indexes. \n",
    "The experiments prints the execution time of different sub-queries that are represented as checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "q = '''\n",
    "SELECT dbalias, t.query, ROUND(exec_time, 2)        \n",
    "FROM single_means_wtimeout as t\n",
    "JOIN queries as q ON q.query = t.query\n",
    "NATURAL JOIN dborder\n",
    "WHERE \n",
    "  -- select class and propoer datasets\n",
    "  q.cls = 'S' AND dataset = 'ldbc' AND\n",
    "  -- filter out non index-capable systems\n",
    "  dbengine <> 'blazegraph'\n",
    "ORDER BY ord, qord\n",
    "'''\n",
    "\n",
    "plt.title('LDBC Macrobench')\n",
    "plt.xlabel('Checkpoints')\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "with get_default_db() as c:\n",
    "    rs = list(c.execute(q)) \n",
    "\n",
    "llabels = None\n",
    "for i, r in enumerate(gby(rs).items()):\n",
    "    db, items = r\n",
    "    ts  = [x[2] for x in items]\n",
    "    if not llabels:\n",
    "        llabels = [x[1][len('ldbc-macrobench-'):] for x in items]\n",
    "    plt.plot(ts, label=db, **styles[i%len(styles)], **markers)\n",
    "     \n",
    "plt.xticks(np.arange(len(llabels)), llabels, rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='time_sum'></a>\n",
    "## Cumulative Time\n",
    "For each system and dataset we sum the running time for each query.\n",
    "We separate running time of queries in isolation and running times of queries in batch mode.\n",
    "\n",
    "**Note:** While all queries can be run in isolation, not all are run in batch mode, only those based on different seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE\n",
    "# On paper they include: CRUDTB\n",
    "# --> no LIS\n",
    "q = '''\n",
    "    SELECT dbalias, dataset, ROUND(SUM(exec_time) / 1000,2)\n",
    "    FROM (\n",
    "        SELECT dbengine, dataset, query,  (exec_time*cnt) as  exec_time\n",
    "        FROM single_means_wtimeout  \n",
    "        NATURAL JOIN queries as q\n",
    "        WHERE dataset IN ({}) AND \n",
    "            cls NOT IN ('L', 'I', 'S') AND\n",
    "            query IN ({})   \n",
    "   ) as v\n",
    "    NATURAL JOIN dborder\n",
    "    NATURAL JOIN queries as q\n",
    "    NATURAL JOIN datasets as dds  \n",
    "    GROUP BY v.dataset, dbengine\n",
    "    ORDER BY ord, dord\n",
    "'''\n",
    "\n",
    "# Avoid the chart be dominated by timeouts on partial runs.\n",
    "queries, queries_labels, in_queries = get_queries(['L', 'I', 'S'])\n",
    "\n",
    "\n",
    "plt.title('All queries'.upper())\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "with get_default_db() as c:\n",
    "    rs = [r for r in c.execute(q.format(in_datasets, in_queries))]\n",
    "for i, r in enumerate(gby(rs).items()):\n",
    "    db, items = r\n",
    "    ts  = [x[2] for x in items]\n",
    "    plt.plot(ts, label=db, **styles[i%len(styles)], **markers)\n",
    "\n",
    "plt.xticks(np.arange(len(xlabels)), xlabels, rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH\n",
    "# On paper they include: CRUDTB\n",
    "# --> no BIL\n",
    "q = '''\n",
    "    SELECT dbalias, dataset, ROUND(SUM(exec_time) / 1000,2)\n",
    "    FROM (\n",
    "        SELECT dbengine,dataset,query, exec_time \n",
    "        FROM bulk_sum_wtimeout\n",
    "        NATURAL JOIN queries as q\n",
    "        WHERE dataset IN ({}) AND \n",
    "            cls NOT IN ('B', 'I', 'L') AND\n",
    "            query IN ({})        \n",
    "   ) as v\n",
    "    NATURAL JOIN dborder\n",
    "    NATURAL JOIN queries as q\n",
    "    NATURAL JOIN datasets as dds  \n",
    "    GROUP BY  v.dataset, dbengine\n",
    "    ORDER BY  ord, dord\n",
    "'''\n",
    "\n",
    "plt.title('Only Batch Queries'.upper())\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "with get_default_db() as c:\n",
    "    rs = [r for r in c.execute(q.format(in_datasets, in_queries))]\n",
    "for i, r in enumerate(gby(rs).items()):\n",
    "    db, items = r\n",
    "    ts  = [x[2] for x in items]\n",
    "    plt.plot(ts, label=db, **styles[i%len(styles)], **markers)\n",
    "\n",
    "plt.xticks(np.arange(len(xlabels)), xlabels, rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index_time'></a>\n",
    "## Effect of indexes\n",
    "\n",
    "The following plots are for queries using indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "q = '''\n",
    "    SELECT dbalias, dataset, ROUND(exec_time, 2)        \n",
    "    FROM single_means_wtimeout as t\n",
    "    JOIN queries as q ON q.query = t.query\n",
    "    NATURAL JOIN dborder\n",
    "    NATURAL JOIN datasets AS ds\n",
    "    WHERE q.query='{}' AND dataset IN ({}) \n",
    "        AND q.cls IN ('I') AND dbengine <> 'blazegraph'\n",
    "    ORDER BY ord, dord\n",
    "'''\n",
    "\n",
    "queries, queries_labels, in_queries = get_queries(['L', 'C', 'R', 'U', 'D', 'T', 'B', 'S'])\n",
    "    \n",
    "xlabels = list(map(ds_short, datasets))\n",
    "current_cls = ''\n",
    "with get_default_db() as c:\n",
    "    for i, x in enumerate(queries):\n",
    "        cls, query = x\n",
    "        if cls != current_cls:\n",
    "            current_cls = cls\n",
    "            display(HTML('<h1 id=\"current_cls\">{}</h1>'.format(current_cls)))\n",
    "            \n",
    "        n, _, qname = queries_labels[i].upper().split(' ', 2)\n",
    "        plt.title('#' + n + ' ' + qname + ' [indexed]')\n",
    "        plt.xlabel('Dataset')\n",
    "        plt.ylabel('Time (ms)')\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True)\n",
    "    \n",
    "        rs = [r for r in c.execute(q.format(query, in_datasets))]\n",
    "        for i, r in enumerate(gby(rs).items()):\n",
    "            db, items = r\n",
    "            ts  = [x[2] for x in items]\n",
    "            plt.plot(ts, label=db, **styles[i%len(styles)], **markers)\n",
    "\n",
    "        plt.xticks(np.arange(len(xlabels)), xlabels, rotation=90)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='disk'></a>\n",
    "## Disk space usage\n",
    "\n",
    "Comparing the increment of size of images for each system, before and after loading the dataset.\n",
    "Raw datasets are loaded in a volume, so this takes into account only the actual system internal storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE 1: we may take care of image size resolution vs dataset size.\n",
    "# A pratical example. Neo4j-tp3 image size resolution is in 10MB (0.01GB),\n",
    "# Yeast is 1.468 KB. The effect of the loading might be hidden in the measurament error,\n",
    "# that implies image_with_data - image_clean = 0.\n",
    "# Since we are using a log scale for the y axis we assum at least a variation of 1MB\n",
    "# from an image_clean to an image_with_data. \n",
    "\n",
    "# NOTE 2: In this version of the chart, the grey area represents the size of the dataset\n",
    "# encoded in JSON on disk; not the number of items in it.\n",
    "\n",
    "q = '''\n",
    "SELECT dbalias, dataset, space\n",
    "FROM (\n",
    "    SELECT dbalias, dataset, MAX(1, im.space - empty.space) AS space, ord, dord\n",
    "    FROM images im\n",
    "    NATURAL JOIN dborder\n",
    "    NATURAL JOIN datasets\n",
    "    JOIN (\n",
    "        SELECT dbengine, space\n",
    "        FROM images \n",
    "        WHERE dataset == 'empty'\n",
    "    ) AS empty ON im.dbengine = empty.dbengine\n",
    "    WHERE dataset IN ({in_datasets})\n",
    "\n",
    "    UNION\n",
    "    \n",
    "    SELECT 'json', dataset, json2, (SELECT COUNT(*) FROM images) + 1, dord\n",
    "    FROM datasets \n",
    "    WHERE dataset IN ({in_datasets})\n",
    ") AS t\n",
    "ORDER BY ord, dord\n",
    "'''\n",
    "\n",
    "with get_default_db() as c:\n",
    "    rs = list(c.execute(q.format(in_datasets=in_datasets)))\n",
    "        \n",
    "plt.title('DISK SPACE USAGE')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Space (MB)')\n",
    "plt.yscale('log', basey=2)\n",
    "plt.grid(True)\n",
    "\n",
    "lst = list(enumerate(gby(rs).items()))\n",
    "for i, r in lst:\n",
    "    db, items = r\n",
    "    ts  = [x[2] for x in items]\n",
    "\n",
    "    if i < (len(lst) -1):\n",
    "        plt.plot(ts, label=db, **styles[i%len(styles)], **markers)\n",
    "    else:\n",
    "        plt.fill_between(np.arange(len(ts)), ts, color='grey', alpha=0.3, label=db)\n",
    "\n",
    "plt.xticks(np.arange(len(xlabels)), xlabels, rotation=90)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='timeouts'></a>\n",
    "## Timeouts\n",
    "We count for each sytem, dataset, and type of query, the number of timeouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Like in paper, indexed and macro-benchmark queries are excluded.\n",
    "# modify the WHERE in the inner query to change that.\n",
    "\n",
    "q = '''\n",
    "SELECT dataset, db, cnt \n",
    "FROM (\n",
    "    SELECT dbengine || '_0' AS db, dataset, SUM(t) - SUM(c) AS cnt, ord, dord \n",
    "    FROM (\n",
    "        SELECT h.dataset, h.dbengine, SUM(h.sid_cnt) c ,SUM(h.expected_cnt) t\n",
    "        FROM health h\n",
    "        LEFT JOIN queries q ON  h.query = q.query\n",
    "        WHERE q.cls <> 'I' AND q.cls <> 'S' AND\n",
    "            h.query IN ({in_queries}) AND\n",
    "            h.dataset IN ({in_datasets})\n",
    "        GROUP BY h.dataset, h.dbengine\n",
    "    ) \n",
    "    NATURAL JOIN dborder\n",
    "    NATURAL JOIN datasets    \n",
    "    GROUP BY dbengine, dataset\n",
    "\n",
    "    UNION\n",
    "\n",
    "    SELECT dbengine || '_1', dataset, SUM(t) - SUM(c), ord, dord \n",
    "    FROM (     \n",
    "        SELECT dataset, dbengine, SUM(ok) as c , COUNT(ok) as t\n",
    "        FROM (\n",
    "            SELECT h.dataset,h.dbengine,h.query, SUM(h.sid_cnt / h.expected_cnt)   AS ok\n",
    "            FROM health_bulk h\n",
    "            JOIN queries q ON  h.query = q.query\n",
    "            WHERE q.cls <> 'I' AND q.cls <> 'S' AND\n",
    "                h.query IN ({in_queries}) AND\n",
    "                h.dataset IN ({in_datasets})\n",
    "            GROUP BY h.dataset,h.dbengine,h.query\n",
    "        )\n",
    "        GROUP BY dataset,dbengine\n",
    "    ) \n",
    "    NATURAL JOIN dborder        \n",
    "    NATURAL JOIN datasets\n",
    "    GROUP BY dbengine, dataset\n",
    ")\n",
    "ORDER BY dord, ord, db\n",
    "'''\n",
    "\n",
    "# NOTE: suffix mapping\n",
    "# 0 -> I\n",
    "# 1 -> B\n",
    "\n",
    "queries, queries_labels, in_queries = get_queries(['I', 'S'])\n",
    "\n",
    "with get_default_db() as c:\n",
    "    rs = list(c.execute(q.format(in_datasets=in_datasets, in_queries=in_queries)))\n",
    "\n",
    "plt.title('TIME-OUTS')\n",
    "plt.xlabel('DB Engine and Execution Method')\n",
    "plt.ylabel('# Timeouts')\n",
    "plt.grid(True)\n",
    "\n",
    "mod_xlabels = ['I\\n' + ('\\n' if i%4 else '') + k[:-2] if not i%2 else 'B' for (i, k) in enumerate(OrderedDict((r[1], None) for r in rs).keys())]\n",
    "for i, r in enumerate(gby(rs).items()):\n",
    "    ds, items = r\n",
    "    timeouts = [x[2] for x in items]\n",
    "    plt.bar(np.arange(len(mod_xlabels)), timeouts, label=ds_short(ds))\n",
    "\n",
    "plt.xticks(np.arange(len(mod_xlabels)), mod_xlabels, rotation=0)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
